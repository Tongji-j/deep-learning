{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加法进位实验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/JerrikEph/jerrikeph.github.io/raw/master/Learn2Carry.png\" width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "import os,sys,tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据生成\n",
    "我们随机在 `start->end`之间采样除整数对`(num1, num2)`，计算结果`num1+num2`作为监督信号。\n",
    "\n",
    "* 首先将数字转换成数字位列表 `convertNum2Digits`\n",
    "* 将数字位列表反向\n",
    "* 将数字位列表填充到同样的长度 `pad2len`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_batch(batch_size, start, end):\n",
    "    '''在(start, end)区间采样生成一个batch的整型的数据\n",
    "    Args :\n",
    "        batch_size: batch_size\n",
    "        start: 开始数值\n",
    "        end: 结束数值\n",
    "    '''\n",
    "    numbers_1 = np.random.randint(start, end, batch_size)\n",
    "    numbers_2 = np.random.randint(start, end, batch_size)\n",
    "    results = numbers_1 + numbers_2\n",
    "    return numbers_1, numbers_2, results\n",
    "\n",
    "def convertNum2Digits(Num):\n",
    "    '''将一个整数转换成一个数字位的列表,例如 133412 ==> [1, 3, 3, 4, 1, 2]\n",
    "    '''\n",
    "    strNum = str(Num)\n",
    "    chNums = list(strNum)\n",
    "    digitNums = [int(o) for o in strNum]\n",
    "    return digitNums\n",
    "\n",
    "def convertDigits2Num(Digits):\n",
    "    '''将数字位列表反向， 例如 [1, 3, 3, 4, 1, 2] ==> [2, 1, 4, 3, 3, 1]\n",
    "    '''\n",
    "    digitStrs = [str(o) for o in Digits]\n",
    "    numStr = ''.join(digitStrs)\n",
    "    Num = int(numStr)\n",
    "    return Num\n",
    "\n",
    "def pad2len(lst, length, pad=0):\n",
    "    '''将一个列表用`pad`填充到`length`的长度 例如 pad2len([1, 3, 2, 3], 6, pad=0) ==> [1, 3, 2, 3, 0, 0]\n",
    "    '''\n",
    "    lst+=[pad]*(length - len(lst))\n",
    "    return lst\n",
    "\n",
    "def results_converter(res_lst):\n",
    "    '''将预测好的数字位列表批量转换成为原始整数\n",
    "    Args:\n",
    "        res_lst: shape(b_sz, len(digits))\n",
    "    '''\n",
    "    res = [reversed(digits) for digits in res_lst]\n",
    "    return [convertDigits2Num(digits) for digits in res]\n",
    "\n",
    "def prepare_batch(Nums1, Nums2, results, maxlen):\n",
    "    '''准备一个batch的数据，将数值转换成反转的数位列表并且填充到固定长度\n",
    "    Args:\n",
    "        Nums1: shape(batch_size,)\n",
    "        Nums2: shape(batch_size,)\n",
    "        results: shape(batch_size,)\n",
    "        maxlen:  type(int)\n",
    "    Returns:\n",
    "        Nums1: shape(batch_size, maxlen)\n",
    "        Nums2: shape(batch_size, maxlen)\n",
    "        results: shape(batch_size, maxlen)\n",
    "    '''\n",
    "    Nums1 = [convertNum2Digits(o) for o in Nums1]\n",
    "    Nums2 = [convertNum2Digits(o) for o in Nums2]\n",
    "    results = [convertNum2Digits(o) for o in results]\n",
    "    \n",
    "    Nums1 = [list(reversed(o)) for o in Nums1]\n",
    "    Nums2 = [list(reversed(o)) for o in Nums2]\n",
    "    results = [list(reversed(o)) for o in results]\n",
    "    \n",
    "    Nums1 = [pad2len(o, maxlen) for o in Nums1]\n",
    "    Nums2 = [pad2len(o, maxlen) for o in Nums2]\n",
    "    results = [pad2len(o, maxlen) for o in results]\n",
    "    \n",
    "    return Nums1, Nums2, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模过程， 按照图示完成建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(myRNNModel, self).__init__()\n",
    "        #self.embed_layer = tf.keras.layers.Embedding(10, 32, batch_input_shape=[None, None])\n",
    "        self.embed_layer = tf.keras.layers.Embedding(10, 32)\n",
    "        \n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(64)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(10)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, num1, num2):\n",
    "        '''\n",
    "        此处完成上述图中模型\n",
    "        '''\n",
    "        # 1. Embedding 映射\n",
    "        x1 = self.embed_layer(num1)  # [batch_size, maxlen, 32]\n",
    "        x2 = self.embed_layer(num2)  # [batch_size, maxlen, 32]\n",
    "\n",
    "        # 2. 两个输入向量按位相加（融合）\n",
    "        x = x1 + x2  # [batch_size, maxlen, 32]\n",
    "\n",
    "        # 3. 输入 RNN 层\n",
    "        out = self.rnn_layer(x)  # [batch_size, maxlen, 64]\n",
    "\n",
    "        # 4. 输出层 -> 每一位输出10类的logits\n",
    "        logits = self.dense(out)  # [batch_size, maxlen, 10]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, y)\n",
    "        loss = compute_loss(logits, label)\n",
    "\n",
    "    # compute gradient\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train(steps, model, optimizer):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step in range(steps):\n",
    "        datas = gen_data_batch(batch_size=200, start=0, end=555555555)\n",
    "        Nums1, Nums2, results = prepare_batch(*datas, maxlen=11)\n",
    "        loss = train_one_step(model, optimizer, tf.constant(Nums1, dtype=tf.int32), \n",
    "                              tf.constant(Nums2, dtype=tf.int32),\n",
    "                              tf.constant(results, dtype=tf.int32))\n",
    "        if step%50 == 0:\n",
    "            print('step', step, ': loss', loss.numpy())\n",
    "\n",
    "    return loss\n",
    "\n",
    "def evaluate(model):\n",
    "    datas = gen_data_batch(batch_size=2000, start=555555555, end=999999999)\n",
    "    Nums1, Nums2, results = prepare_batch(*datas, maxlen=11)\n",
    "    logits = model(tf.constant(Nums1, dtype=tf.int32), tf.constant(Nums2, dtype=tf.int32))\n",
    "    logits = logits.numpy()\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    res = results_converter(pred)\n",
    "    for o in list(zip(datas[2], res))[:20]:\n",
    "        print(o[0], o[1], o[0]==o[1])\n",
    "\n",
    "    print('accuracy is: %g' % np.mean([o[0]==o[1] for o in zip(datas[2], res)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(0.001)\n",
    "model = myRNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\33552\\AppData\\Local\\Temp\\ipykernel_5292\\824112904.py\", line 15, in train_one_step  *\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    File \"c:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 383, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"c:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 436, in apply\n        grads, trainable_variables = self._filter_empty_gradients(\n    File \"c:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 772, in _filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable.\")\n\n    ValueError: No gradients provided for any variable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m evaluate(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(steps, model, optimizer)\u001b[39m\n\u001b[32m     22\u001b[39m datas = gen_data_batch(batch_size=\u001b[32m200\u001b[39m, start=\u001b[32m0\u001b[39m, end=\u001b[32m555555555\u001b[39m)\n\u001b[32m     23\u001b[39m Nums1, Nums2, results = prepare_batch(*datas, maxlen=\u001b[32m11\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m loss = \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNums1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNums2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step%\u001b[32m50\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m, step, \u001b[33m'\u001b[39m\u001b[33m: loss\u001b[39m\u001b[33m'\u001b[39m, loss.numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\__autograph_generated_filel12pzwvy.py:14\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_one_step\u001b[39m\u001b[34m(model, optimizer, x, y, label)\u001b[39m\n\u001b[32m     12\u001b[39m     loss = ag__.converted_call(ag__.ld(compute_loss), (ag__.ld(logits), ag__.ld(label)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     13\u001b[39m grads = ag__.converted_call(ag__.ld(tape).gradient, (ag__.ld(loss), ag__.ld(model).trainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:383\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m    382\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterations\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:436\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    429\u001b[39m grads, trainable_variables = (\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m._overwrite_variables_directly_with_gradients(\n\u001b[32m    431\u001b[39m         grads, trainable_variables\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m )\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m grads, trainable_variables = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) == \u001b[32m0\u001b[39m:\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:772\u001b[39m, in \u001b[36mBaseOptimizer._filter_empty_gradients\u001b[39m\u001b[34m(self, grads, vars)\u001b[39m\n\u001b[32m    769\u001b[39m             missing_grad_vars.append(v.name)\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo gradients provided for any variable.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[32m    774\u001b[39m     warnings.warn(\n\u001b[32m    775\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGradients do not exist for variables \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    776\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when minimizing the loss.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    777\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    778\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`loss` argument?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    779\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"C:\\Users\\33552\\AppData\\Local\\Temp\\ipykernel_5292\\824112904.py\", line 15, in train_one_step  *\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    File \"c:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 383, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"c:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 436, in apply\n        grads, trainable_variables = self._filter_empty_gradients(\n    File \"c:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 772, in _filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable.\")\n\n    ValueError: No gradients provided for any variable.\n"
     ]
    }
   ],
   "source": [
    "train(3000, model, optimizer)\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
